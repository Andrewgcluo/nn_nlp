{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 简介\n",
    "\n",
    "本 note 中主要实现神经网络中常见的函数，所有函数的实现均考虑向量计算，包括：\n",
    "\n",
    "1. `sigmoid(x)`\n",
    "2. `relu(x)`\n",
    "3. `softmax(x)`\n",
    "4. `cross_entropy_entry(y, t)` `y`是预测标签，`t`是实际数据中标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. sigmoid\n",
    "\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 任意维度的张量\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    施加 sigmoid 计算后的 x\n",
    "    '''\n",
    "    \n",
    "    return 1 / 1 + np.exp(-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. relu\n",
    "\n",
    "`relu()` 的实现主要是将其行为用 `maximum(0, x)` 函数来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 任意维度的张量\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    施加 relu 计算后的 x\n",
    "    '''\n",
    "    return np.maximum(0, x)  # 取 x 和 0 的各个位置上较大的元素返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. softmax\n",
    "\n",
    "`softmax` 的实现要点如下：\n",
    "\n",
    "1. 数值计算的问题，即计算 `exp(a)`时，如果`a` 过大，会出现 `inf` 的情况，两个 `inf` 做除法会出现不确定的情况，而 $C\\times \\exp(x) = \\exp(x + \\ln{C})$，所以可以先对 `x` 中的元素先减去对应的最大值避免 `inf` 的情况出现\n",
    "2. 根据输入数据是向量还是 mini-batch 对应的二维矩阵，分别进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Paramters\n",
    "    ---------\n",
    "    x : 向量或者 mini-batch 下的矩阵，每一行对应一条数据\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    施加 softmax 计算后的 x\n",
    "    '''\n",
    "    \n",
    "    ## 先检测是否是 mini-batch 的情况\n",
    "    ## mini-batch 需要每一行单独处理\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True) # 各条数据分别减去各自所有元素中的最大值\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "        \n",
    "    elif x.ndim == 1:  # 单条数据\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. cross_entropy_error\n",
    "\n",
    "交叉熵误差的实现主要是区分输入数据 `y` 和 `t` 是 one-hot 形式给出还是直接以单独的标签数字给出，可以任意选择以何种格式作为计算方式\n",
    "\n",
    "另外，为了保证 `log(x)` 中的 $x$ 不接近于 $0$，可以加上一个足够小的值，即 `log(x + 1e-7)`，一般情况下 `1e-7` 会在浮点数的近似表示中被“吸收”\n",
    "\n",
    "输入数据默认按 mini-batch 处理，其中每一行代表一条数据，因为单条数据属于 mini-batch 的特例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corss_entropy_error(y, t):\n",
    "    '''\n",
    "    Paramters\n",
    "    ---------\n",
    "    y : 预测标签\n",
    "    t : 正确标签\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    交叉熵误差\n",
    "    '''\n",
    "    \n",
    "    ## 标签数据转 one-hot 格式\n",
    "    ## 判断是否是标签格式数据\n",
    "    if t.ndim == 1:  # 标签格式，进行转换\n",
    "        y = y.reshape(1, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
