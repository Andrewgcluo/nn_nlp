{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 简介\n",
    "\n",
    "本 note 中主要实现神经网络中常见的函数，所有函数的实现均考虑向量计算，包括：\n",
    "\n",
    "1. `sigmoid(x)` - 处理格式张量\n",
    "2. `relu(x)` - 处理任意格式张量\n",
    "3. `softmax(x)` - 向量数据或者向量数据组成的 mini-batch 数据\n",
    "4. `cross_entropy_entry(y, t)` - - 向量数据或者向量数据组成的 mini-batch 数据，其中`y`是预测标签，`t`是实际标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. sigmoid\n",
    "\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 任意维度的张量\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    施加 sigmoid 计算后的 x\n",
    "    '''\n",
    "    \n",
    "    return 1 / 1 + np.exp(-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. relu\n",
    "\n",
    "`relu()` 的实现主要是将其行为用 `maximum(0, x)` 函数来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 任意维度的张量\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    施加 relu 计算后的 x\n",
    "    '''\n",
    "    return np.maximum(0, x)  # 取 x 和 0 的各个位置上较大的元素返回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. softmax\n",
    "\n",
    "`softmax` 的实现要点如下：\n",
    "\n",
    "1. 数值计算的问题，即计算 `exp(a)`时，如果`a` 过大，会出现 `inf` 的情况，两个 `inf` 做除法会出现不确定的情况，而 $C\\times \\exp(x) = \\exp(x + \\ln{C})$，所以可以先对 `x` 中的元素先减去对应的最大值避免 `inf` 的情况出现\n",
    "2. 根据输入数据是向量还是 mini-batch 对应的二维矩阵，分别进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Paramters\n",
    "    ---------\n",
    "    x : 向量或者 mini-batch 下的矩阵，每一行对应一条数据\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    施加 softmax 计算后的 x\n",
    "    '''\n",
    "    \n",
    "    ## 先检测是否是 mini-batch 的情况\n",
    "    ## mini-batch 需要每一行单独处理\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True) # 各条数据分别减去各自所有元素中的最大值\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "        \n",
    "    elif x.ndim == 1:  # 单条数据\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. cross_entropy_error\n",
    "\n",
    "交叉熵误差的实现要点在于：\n",
    "\n",
    "0. 需要区分训练数据中 `t` 是 one-hot 形式给出还是直接以单独的标签数字给出，相比标签数据，one-hot 格式需要做一步解码索引的操作来确定具体的标签值，最后统一转换为 one-hot 格式来处理\n",
    "\n",
    "1. 因为 `y` 来自于上一层类似于 `softmax()` 的输出结果，所以 `y` 一定不是一个单一的数据标签\n",
    "\n",
    "2. 如果 `t.size == y.size` 根据 1，`t` 一定是 one-hot 格式\n",
    "\n",
    "3. 因为 one-hot 格式的特殊性，只有 t 中为 1 的位置对应的元素才会最终算入交叉熵误差中，所以可以用索引 y 在正确输出位置的值来计算交叉熵，即 `y[np.arange(batch_size), t]`\n",
    "\n",
    "3. 为了保证 `log(x)` 中的 $x$ 不接近于 $0$，可以加上一个足够小的值，即 `log(x + 1e-7)`，一般情况下 `1e-7` 会在浮点数的近似表示中被“吸收”\n",
    "\n",
    "4. 输入数据默认按 mini-batch 处理，其中每一行代表一条数据，因为单条数据属于 mini-batch 的特例，需要将其格式转换为 mini-batch 的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corss_entropy_error(y, t):\n",
    "    '''\n",
    "    Paramters\n",
    "    ---------\n",
    "    y : 预测标签\n",
    "    t : 正确标签\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    交叉熵误差\n",
    "    '''\n",
    "    \n",
    "    ## 如果是单条数据，也转换为 mini-batch 形式\n",
    "    if y.ndim == 1:  # 标签格式，转换为行向量形式\n",
    "        y = y.reshape(1, y.size)\n",
    "        t = t.reshape(1, t.size)\n",
    "\n",
    "    ## 数据是 one-hot 形式\n",
    "    if t.size == y.size:  # t 是 one-hot 格式\n",
    "        t = t.argmax(axis=1)  # 取回 t 的标签，因为只有标签处的值才是 1，其他处为 0\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
