{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Intro\n",
    "\n",
    "本 note 关注于实现神经网络中常用的层，设计原则如下：\n",
    "\n",
    "1. 每一层均使用一个类来实现\n",
    "2. 每一个类维护前向计算时所需要的**参数**、后向计算时所需要的**中间结果**、更新参数时所需要的**梯度**\n",
    "3. 每个类的**参数**和**梯度**用列表 `params` 和 `grads` 打包，便于整个神经网络的训练\n",
    "4. 每一个类至少包含三个方法，即 `__init__()`、`forward()`、`backward()`\n",
    "5. 复杂类可以基于简单类来实现\n",
    "\n",
    "**备注**\n",
    "> 梯度的形状和对应参数的形状一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MatMul Layer\n",
    "\n",
    "本层执行矩阵乘法运算，即 $\\mathbf{y} = \\mathbf{x}\\mathbf{W}$，这里不考虑偏置值\n",
    "\n",
    "实现如下：\n",
    "\n",
    "**成员变量**\n",
    "- 参数： `W` - 参数矩阵\n",
    "- 中间结果 `x` - 输入数据\n",
    "- 梯度: `dW`\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self, W)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout)`\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        '''初始化所需要维护的成员变量，其中参数 W 由外界传入\n",
    "        '''\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''前向计算\n",
    "        \n",
    "        :param x: 是输入数据\n",
    "        \n",
    "        :return: 输出矩阵乘法的结果\n",
    "        '''\n",
    "        W, = self.params\n",
    "        self.x = x\n",
    "        out = np.dot(x, W)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''后向计算，计算更新本层参数的梯度，以及传播到后一层的导数\n",
    "        \n",
    "        :param dout: 上游来的导数信号\n",
    "        \n",
    "        :return dx: 传给下游的导数信号\n",
    "        '''\n",
    "        ## 这里考虑了 x 是批数据的情况\n",
    "        W, = self.params\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        \n",
    "        self.grads[0][...] = dW  # 注意这里的深拷贝\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Affine Layer\n",
    "\n",
    "Affine 层实现的仿射变换，即 $\\mathbf{y}=\\mathbf{x}\\mathbf{W} + \\mathbf{b}$，相比 MatMul，Affine 多了一个偏置值的计算\n",
    "\n",
    "实现如下：\n",
    "\n",
    "**成员变量**\n",
    "\n",
    "- 参数：`W` - 矩阵； `b` - 偏置值;\n",
    "- 中间结果: `x` - 输入数据\n",
    "- 梯度: `dW`；`db`\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self, W, b)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout)`\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        '''初始化 Affine 所维护的参数，参数 W 和 b 由外界传入\n",
    "        '''\n",
    "        self.params = [W, b]  # 参数列表\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]  # 梯度列表\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''前向计算\n",
    "        '''\n",
    "        W, b = self.params\n",
    "        self.x = x\n",
    "        out = np.dot(x, W) + b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''后向计算，计算更新本层参数所需的梯度，以及下游所需的导数信号\n",
    "        '''\n",
    "        W, b = self.params\n",
    "        \n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Softmax Layer\n",
    "\n",
    "softmax 层完成的是如下的计算 $\\mathbf{y} = softmax(\\mathbf{x})$\n",
    "\n",
    "不失一般性，这里仅考虑 $\\mathbf{x}$ 是一维向量或者二维批数据的情况\n",
    "\n",
    "**成员变量** （准确讲，softmax 没有需要学习的参数，为了可以统一处理，使用空 `params` 和 `grads` 来替代）\n",
    "\n",
    "- `out` - 输出结果\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout)`\n",
    "\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 SoftmaxWithLoss Layer\n",
    "\n",
    "本层可以看做是 Softmax 层后接一个交叉熵计算所形成的复合层，softmax + 交叉熵是比较常见的用于多分类问题的组合\n",
    "\n",
    "实现如下：\n",
    "\n",
    "**成员变量**\n",
    "\n",
    "- 没有需要学习的参数，故 `params` 和 `grads` 均是空列表\n",
    "- 中间结果 `y` - softmax 的输出；`t` - 训练数据中的标签\n",
    "- 梯度 -  `grads` 为空列表\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout=1)` - 因为一般作为神经网络的输出，所以 `dout=1`\n",
    "\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []  # 没有需要学习的参数，用空列表代替便于处理的一致性\n",
    "        self.y = None  # softmax 的输出\n",
    "        self.t = None  # 训练数据中的标签\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.y = softmax(x)\n",
    "        self.t = t\n",
    "        \n",
    "        ## 如果 t 是 one-hot 标签，转换为正确标签的索引值\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "        \n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        '''后向传播计算，因为本层一般是作为模型的输出层，所以上游来的导数默认为1\n",
    "        '''\n",
    "        ## 默认作为 mini-batch 来处理\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        ## 在推导 dx = dout * (y-t)，默认 t 是以 one-hot 的格式出现\n",
    "        ## 所以在计算 y-t 时，可以只计算正确标签对应位置的减法，因为其他位置 t 元素值为 0\n",
    "        ## 即只要拿到正确标签的索引值即可\n",
    "        ## 这里需要先处理 one-hot 格式的 t，以获取正确标签的索引值\n",
    "        \n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "            \n",
    "        dx = self.y.copy()  # 深拷贝一份 y\n",
    "        dx[np.arange(batch_size), self.t] -= 1  # 计算 y-t 的技巧\n",
    "        dx = dx * dout\n",
    "        dx = dx / batch_size  # 共 batch-size 个数据参与计算交叉熵，每条数据贡献需要除以数据量\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Sigmoid Layer\n",
    "\n",
    "sigmoid 层完成的计算如下 $\\mathbf{y} = \\frac{1}{1 + \\exp(-\\mathbf{x})}$\n",
    "\n",
    "sigmoid 一般用于完成二分类问题，实现如下：\n",
    "\n",
    "**成员变量**\n",
    "\n",
    "- 参数 - 没有需要学习的参数，故 `params`是空列表\n",
    "- 中间变量 - `out` 前向计算的输出 out\n",
    "- 梯度 -  `grads` 为空列表\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout)`\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / 1 + np.exp(-x)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
