{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Intro\n",
    "\n",
    "本 note 关注于实现神经网络中常用的层，设计原则如下：\n",
    "\n",
    "1. 每一层均使用一个类来实现\n",
    "2. 每一个类维护前向计算时所需要的**参数**、后向计算时所需要的**中间结果**、更新参数时所需要的**梯度**\n",
    "3. 每个类的**参数**和**梯度**用列表 `params` 和 `grads` 打包，便于整个神经网络的训练\n",
    "4. 每一个类至少包含三个方法，即 `__init__()`、`forward()`、`backward()`\n",
    "5. 复杂类可以基于简单类来实现\n",
    "\n",
    "**备注**\n",
    "> 梯度的形状和对应参数的形状一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MatMul Layer\n",
    "\n",
    "本层执行矩阵乘法运算，即 $\\mathbf{y} = \\mathbf{x}\\mathbf{W}$，这里不考虑偏置值\n",
    "\n",
    "实现如下：\n",
    "\n",
    "**成员变量**\n",
    "- 参数： `W` - 参数矩阵\n",
    "- 中间结果 `x` - 输入数据\n",
    "- 梯度: `dW`\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self, W)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout)`\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        '''初始化所需要维护的成员变量，其中参数 W 由外界传入\n",
    "        '''\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''前向计算\n",
    "        \n",
    "        :param x: 是输入数据\n",
    "        \n",
    "        :return: 输出矩阵乘法的结果\n",
    "        '''\n",
    "        W, = self.params\n",
    "        self.x = x\n",
    "        out = np.dot(x, W)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''后向计算，计算更新本层参数的梯度，以及传播到后一层的导数\n",
    "        \n",
    "        :param dout: 上游来的导数信号\n",
    "        \n",
    "        :return dx: 传给下游的导数信号\n",
    "        '''\n",
    "        ## 这里考虑了 x 是批数据的情况\n",
    "        W, = self.params\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        \n",
    "        self.grads[0][...] = dW  # 注意这里的深拷贝\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Affine Layer\n",
    "\n",
    "Affine 层实现的仿射变换，即 $\\mathbf{y}=\\mathbf{x}\\mathbf{W} + \\mathbf{b}$，相比 MatMul，Affine 多了一个偏置值的计算\n",
    "\n",
    "实现如下：\n",
    "\n",
    "**成员变量**\n",
    "\n",
    "- 参数：`W` - 矩阵； `b` - 偏置值;\n",
    "- 中间结果: `x` - 输入数据\n",
    "- 梯度: `dW`；`db`\n",
    "\n",
    "**成员函数**\n",
    "\n",
    "- `__init__(self, W, b)`\n",
    "- `forward(self, x)`\n",
    "- `backward(self, dout)`\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        '''初始化 Affine 所维护的参数，参数 W 和 b 由外界传入\n",
    "        '''\n",
    "        self.params = [W, b]  # 参数列表\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]  # 梯度列表\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''前向计算\n",
    "        '''\n",
    "        W, b = self.params\n",
    "        self.x = x\n",
    "        out = np.dot(x, W) + b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''后向计算，计算更新本层参数所需的梯度，以及下游所需的导数信号\n",
    "        '''\n",
    "        W, b = self.params\n",
    "        \n",
    "        dW\n",
    "        db\n",
    "        dx = s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
